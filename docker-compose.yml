version: "3"

services:
  spark:
    build:
      context: .
      dockerfile: ./container/Dockerfile
    volumes:
      - ./src:/opt/project
    ports:
      - 4040:4040
      - 7077:7077
      - 8081:8081
      - 8080:8080
      - 18080:18080
    restart: unless-stopped
    command: sh /run.sh
  # zookeeper:
  #   image: confluentinc/cp-zookeeper:5.2.0
  #   hostname: zookeeper
  #   ports:
  #     - "2181:2181"
  #   expose:
  #     - "2181"
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #   restart: unless-stopped
  # kafka:
  #   image: confluentinc/cp-kafka:5.2.0
  #   hostname: kafka
  #   ports:
  #     - "9092:9092"
  #   expose:
  #     - "9092"
  #   depends_on:
  #     - zookeeper
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
  #     KAFKA_AUTO_CREATE_TOPICS_ENABLE: "True"
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
  #   restart: unless-stopped
  # namenode:
  #   image: bde2020/hadoop-namenode:2.0.0-hadoop3.1.1-java8
  #   container_name: namenode
  #   environment:
  #     - CLUSTER_NAME=test
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  #   env_file:
  #     - ./hadoop-hive.env
  #   ports:
  #     - 50070:50070
  # datanode:
  #   image: bde2020/hadoop-datanode:2.0.0-hadoop3.1.1-java8
  #   container_name: datanode
  #   depends_on:
  #     - namenode
  #   environment:
  #     - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
  #   env_file:
  #     - ./hadoop-hive.env
  #   ports:
  #     - 50075:50075
  # hue:
  #   image: bde2020/hdfs-filebrowser:3.11
  #   container_name: hue
  #   ports:
  #     - 8088:8088
  #   environment:
  #     - NAMENODE_HOST=namenode
